{"cells":[{"cell_type":"markdown","metadata":{"id":"XBITN0M_LKds"},"source":["# HW2P2: Face Classification and Verification\n"]},{"cell_type":"markdown","metadata":{"id":"-NH4P-HzLRQs"},"source":["Congrats on coming to the second homework in 11785: Introduction to Deep Learning. This homework significantly longer and tougher than the previous homework. You have 2 sub-parts as outlined below. Please start early!\n","\n","\n","*   Face Recognition: You will be writing your own CNN model to tackle the problem of classification, consisting of 7001 identities\n","*   Face Verification: You use the model trained for classification to evaluate the quality of its feature embeddings, by comparing the similarity of known and unknown identities"]},{"cell_type":"markdown","metadata":{"id":"i1B_m84_cU6c"},"source":["Common errors which you may face in this homeworks (because of the size of the model)\n","\n","\n","*   CUDA Out of Memory (OOM): You can tackle this problem by (1) Reducing the batch size (2) Calling `torch.cuda.empty_cache()` and `gc.collect()` (3) Finally restarting the runtime\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["# FINAL MODEL ARICHTECTURE USED: SEResNet (Squeeze-and-Excitation Residual Network)\n","The SEResNet is an architectural variant of the popular Residual Network (ResNet) that incorporates attention mechanisms. It was proposed to improve the performance of convolutional neural networks by enhancing feature interactions and adaptively reweighting feature maps.\n","\n","Here are the key components of SE-ResNet:\n","\n","Residual Blocks: SE-ResNet employs residual blocks similar to the standard ResNet architecture. These blocks consist of a stack of convolutional layers, batch normalization, and ReLU activations.\n","\n","Squeeze-and-Excitation (SE) Module: The distinctive feature of SE-ResNet is the SE module. It's introduced within each residual block and aims to capture channel-wise dependencies and adaptively recalibrate the feature maps. The SE module has two steps:\n","\n","Squeeze: Global Average Pooling (GAP) is applied to reduce the spatial dimensions of feature maps into a single value per channel. This step compresses the channel-wise information.\n","Excitation: A fully connected network (usually a couple of dense layers) is applied to the output of the squeeze step. This network learns to assign a weight to each channel.\n","Skip Connections: Skip connections are preserved throughout the network, allowing gradients to flow easily during training. The combination of residual blocks and skip connections helps in mitigating the vanishing gradient problem.\n","\n","The SE-ResNet architecture has demonstrated improved performance in various computer vision tasks, such as image classification, object detection, and semantic segmentation. By incorporating the SE module, the network can adaptively focus on more informative channels and suppress less relevant ones, making it more efficient and accurate."]},{"cell_type":"markdown","metadata":{},"source":["## SEResNet explanation in terms of face classification and verification"]},{"cell_type":"markdown","metadata":{},"source":["SEResNet, or Squeeze-and-Excitation Residual Network, is a convolutional neural network architecture that incorporates Squeeze-and-Excitation (SE) blocks to enhance feature recalibration and improve network performance. \n","In the context of face classification and verification, SEResNet will be applied as follow:\n","\n","1. Face Classification typically involves the task of categorizing faces into predefined classes, such as identifying individuals based on their facial features.\n","SEResNet can be used as a deep learning model for face classification tasks. It excels at learning discriminative features from facial images, making it effective for identifying people in images.\n","The Squeeze-and-Excitation blocks within SEResNet help the network focus on important facial features while reducing the impact of less relevant details, thus improving the accuracy of face classification.\n","\n","2. Face Verification is the task of determining whether two facial images belong to the same individual or not. It's often used for applications like face-based access control or user authentication.\n","SEResNet can be used in face verification by training it to learn feature representations of faces and then comparing these representations to determine if the faces are from the same person or not.\n","The SE blocks in SEResNet can help capture subtle differences and similarities in facial features, making it suitable for fine-grained face verification tasks.\n","\n","Overall, in both face classification and verification scenarios, SEResNet benefits from its ability to automatically recalibrate features within the network using the SE (Squeeze-and-Excitation ) blocks. This recalibration helps focus on important facial characteristics, which is critical for achieving high accuracy in these tasks. However, it's essential to note that the model's performance is highly dependent on the quality and diversity of the training data, as well as fine-tuning and hyperparameter tuning to suit the specific requirements of the face classification or verification task."]},{"cell_type":"markdown","metadata":{},"source":["This project aims was to explore CNN arichtectures while performing this task of face classification and verification. I learned hyperparameter tuning to measure that the model perform well.\n","\n","Hyperparameters used and resulted in the best score are:\n","300 epochs used, batch_size of 128, learning rate of 1e-3, rotation_angle of 30, and horizontal_flip of 0.5.\n","\n","Data loader functions were optimized using transformation such as rotation, Augmentation, horizontal flipping, etc. All these techniques used to improve the goodness of the data so that the model will generalize well the output.\n","# Results\n","\n","classification task Training accuracy was 99.89%\n","classification task validation accuracy was 90.346%\n","\n","verification task  accuracy was 54.44%\n","verification task validation accuracy was 56.944%\n","\n","# Fine tuning \n","Different trial made: batch size: 64,256,128 Roration angle:25,30, Lr = 0.1,0.0001,0.001; brightness: 0.2,0.2,0.2\n","\n","# code running\n","\n","To ensure the code are running, Kaggle can be used with GPU runtime.\n","To run the code cells, start from the beginning."]},{"cell_type":"markdown","metadata":{"id":"BdoDIKWOMF59"},"source":["# Preliminaries"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:16:21.242796Z","iopub.status.busy":"2023-10-23T02:16:21.242374Z","iopub.status.idle":"2023-10-23T02:16:22.265143Z","shell.execute_reply":"2023-10-23T02:16:22.264016Z","shell.execute_reply.started":"2023-10-23T02:16:21.242765Z"},"id":"Jza7lwiScUhb","outputId":"c2846eb5-7aaa-4448-bc7e-700ec0f81bad","trusted":true},"outputs":[],"source":["!nvidia-smi # to see what GPU you have"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:16:22.267820Z","iopub.status.busy":"2023-10-23T02:16:22.267422Z","iopub.status.idle":"2023-10-23T02:16:33.936991Z","shell.execute_reply":"2023-10-23T02:16:33.935777Z","shell.execute_reply.started":"2023-10-23T02:16:22.267781Z"},"id":"bTxfd_nqFnL9","outputId":"6a5c6f16-8fec-4755-abad-8153aac2cf3a","trusted":true},"outputs":[],"source":["!pip install wandb --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:16:33.938635Z","iopub.status.busy":"2023-10-23T02:16:33.938319Z","iopub.status.idle":"2023-10-23T02:16:45.798792Z","shell.execute_reply":"2023-10-23T02:16:45.797461Z","shell.execute_reply.started":"2023-10-23T02:16:33.938606Z"},"trusted":true},"outputs":[],"source":["!pip install --upgrade wandb --quiet"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:16:45.802494Z","iopub.status.busy":"2023-10-23T02:16:45.802055Z","iopub.status.idle":"2023-10-23T02:16:45.812467Z","shell.execute_reply":"2023-10-23T02:16:45.811416Z","shell.execute_reply.started":"2023-10-23T02:16:45.802452Z"},"id":"jwLEd0gdPbSc","outputId":"bd8f7edf-c0ea-44b9-c7e4-93f0fd5bbf85","trusted":true},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","# from torchsummary import summary\n","import torchvision #This library is used for image-based operations (Augmentations)\n","import os\n","import gc\n","from tqdm import tqdm\n","from PIL import Image\n","import numpy as np\n","import pandas as pd\n","from sklearn.metrics import accuracy_score\n","import glob\n","import io\n","import wandb\n","import matplotlib.pyplot as plt\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","print(\"Device: \", DEVICE)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# from google.colab import drive # Link your drive if you are a colab user\n","# drive.mount('/content/drive') # Models in this HW take a long time to get trained and make sure to save it her"]},{"cell_type":"markdown","metadata":{"id":"1oxQNl-YVWHc"},"source":["# TODOs\n","As you go, please read the code and keep an eye out for TODOs!"]},{"cell_type":"markdown","metadata":{"id":"scOnMklwWBY6"},"source":["# Download Data from Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:16:45.814000Z","iopub.status.busy":"2023-10-23T02:16:45.813662Z","iopub.status.idle":"2023-10-23T02:16:49.969131Z","shell.execute_reply":"2023-10-23T02:16:49.968029Z","shell.execute_reply.started":"2023-10-23T02:16:45.813974Z"},"id":"6BksgPdkQwwb","outputId":"f0cd3d14-f0b9-4c58-d383-168420c51c3f","trusted":true},"outputs":[],"source":["# TODO: Use the same Kaggle code from HW1P2\n","!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n","!mkdir /root/.kaggle\n","\n","with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n","    f.write('{\"username\":\"umuhozaalice\",\"key\":\"9d299ae7b28d6e93d785c9337c271ec0\"}')\n","    # Put your kaggle username & key here\n","\n","!chmod 600 /root/.kaggle/kaggle.json"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# !mkdir '/content/data'\n","\n","# !kaggle competitions download -c 11-785-f23-hw2p2-classification\n","# !unzip -qo '11-785-f23-hw2p2-classification.zip' -d '/content/data'\n","\n","# !kaggle competitions download -c 11-785-f23-hw2p2-verification\n","# !unzip -qo '11-785-f23-hw2p2-verification.zip' -d '/content/data'"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:16:49.971327Z","iopub.status.busy":"2023-10-23T02:16:49.970920Z","iopub.status.idle":"2023-10-23T02:16:58.097845Z","shell.execute_reply":"2023-10-23T02:16:58.096745Z","shell.execute_reply.started":"2023-10-23T02:16:49.971288Z"},"trusted":true},"outputs":[],"source":["# !mkdir '/kaggle/working'\n","!kaggle competitions download -c 11-785-f23-hw2p2-classification\n","!unzip -qo '11-785-f23-hw2p2-classification.zip' -d '/kaggle/working'\n","\n","!kaggle competitions download -c 11-785-f23-hw2p2-verification\n","!unzip -qo '11-785-f23-hw2p2-verification.zip' -d '/kaggle/working'"]},{"cell_type":"markdown","metadata":{"id":"O68hT27SXClj"},"source":["# Configs"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:16:58.099906Z","iopub.status.busy":"2023-10-23T02:16:58.099471Z","iopub.status.idle":"2023-10-23T02:16:58.105517Z","shell.execute_reply":"2023-10-23T02:16:58.104556Z","shell.execute_reply.started":"2023-10-23T02:16:58.099865Z"},"id":"S7qpMxG0XCJz","trusted":true},"outputs":[],"source":["config = {\n","    'batch_size': 128,  # Increase this if your GPU can handle it\n","    'lr': 0.001,       \n","    'epochs': 300,      # 20 epochs is recommended ONLY for the early submission - you will have to train for much longer typically\n","    'rotation_angle': 30,\n","    'horizontal_flip': 0.5,\n","    'brightness': 0.15,\n","    'contrast': 0.15,\n","    'saturation': 0, \n","    'hue': 0,\n","    # Include other parameters as needed.\n","} \n"]},{"cell_type":"markdown","metadata":{"id":"sSeiKHYrM-6b"},"source":["# Classification Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:16:58.106950Z","iopub.status.busy":"2023-10-23T02:16:58.106699Z","iopub.status.idle":"2023-10-23T02:17:24.223531Z","shell.execute_reply":"2023-10-23T02:17:24.222729Z","shell.execute_reply.started":"2023-10-23T02:16:58.106928Z"},"id":"tmRX5omaNDEZ","outputId":"a0b55a39-9ad9-4081-9230-bc5ffc3c5235","trusted":true},"outputs":[],"source":["DATA_DIR    = '/kaggle/working/11-785-f23-hw2p2-classification'# TODO: Path where you have downloaded the data\n","TRAIN_DIR   = os.path.join(DATA_DIR, \"train\")\n","VAL_DIR     = os.path.join(DATA_DIR, \"dev\")\n","TEST_DIR    = os.path.join(DATA_DIR, \"test\")\n","\n","\n","\n","# Transforms using torchvision - Refer https://pytorch.org/vision/stable/transforms.html\n","\n","train_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ColorJitter(brightness=config['brightness'], contrast=config['contrast'], saturation=config['saturation'], hue=config['hue']),\n","    torchvision.transforms.RandomPerspective(0.4, 0.4),\n","    torchvision.transforms.RandomRotation(degrees=config['rotation_angle']),\n","    torchvision.transforms.RandomHorizontalFlip(p=config['horizontal_flip']),\n","    torchvision.transforms.RandAugment(3,8),\n","    torchvision.transforms.ToTensor() ])# Implementing the right train transforms/augmentation methods is key to improving performance.\n","\n","# Most torchvision transforms are done on PIL images. So you convert it into a tensor at the end with ToTensor()\n","# But there are some transforms which are performed after ToTensor() : e.g - Normalization\n","# Normalization Tip - Do not blindly use normalization that is not suitable for this dataset\n","\n","valid_transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor()\n","])\n","\n","\n","train_dataset   = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms)\n","valid_dataset   = torchvision.datasets.ImageFolder(VAL_DIR, transform= valid_transforms)\n","# You should NOT have data augmentation on the validation set. Why?\n","# The goal of data augmentation is to improve the model's ability to generalize and perform well on unseen data \n","# by exposing it to a wider variety of input variations during training.\n","# The validation set, on the other hand, is used to estimate how well the model generalizes to unseen data. \n","# It serves as an independent dataset to evaluate the model's performance.\n","# Overall, to properly assess a model's performance and its ability to generalize to real-world scenarios, \n","# it's important to keep the validation set separate and unaltered, without any form of data augmentation.\n","\n","\n","# Create data loaders\n","train_loader = torch.utils.data.DataLoader(\n","    dataset     = train_dataset,\n","    batch_size  = config['batch_size'],\n","    shuffle     = True,\n","    num_workers = 2,\n","    pin_memory  = True\n",")\n","\n","valid_loader = torch.utils.data.DataLoader(\n","    dataset     = valid_dataset,\n","    batch_size  = config['batch_size'],\n","    shuffle     = False,\n","    num_workers = 2\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:17:24.225034Z","iopub.status.busy":"2023-10-23T02:17:24.224737Z","iopub.status.idle":"2023-10-23T02:17:24.231541Z","shell.execute_reply":"2023-10-23T02:17:24.230666Z","shell.execute_reply.started":"2023-10-23T02:17:24.225009Z"},"id":"SqSR063BGE2e","trusted":true},"outputs":[],"source":["# You can do this with ImageFolder as well, but it requires some tweaking\n","class ClassificationTestDataset(torch.utils.data.Dataset):\n","\n","    def __init__(self, data_dir, transforms):\n","        self.data_dir   = data_dir\n","        self.transforms = transforms\n","\n","        # This one-liner basically generates a sorted list of full paths to each image in the test directory\n","        self.img_paths  = list(map(lambda fname: os.path.join(self.data_dir, fname), sorted(os.listdir(self.data_dir))))\n","\n","    def __len__(self):\n","        return len(self.img_paths)\n","\n","    def __getitem__(self, idx):\n","        return self.transforms(Image.open(self.img_paths[idx]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:17:24.235354Z","iopub.status.busy":"2023-10-23T02:17:24.235092Z","iopub.status.idle":"2023-10-23T02:17:24.841206Z","shell.execute_reply":"2023-10-23T02:17:24.840220Z","shell.execute_reply.started":"2023-10-23T02:17:24.235331Z"},"id":"fVLB41KtGC2o","trusted":true},"outputs":[],"source":["test_dataset = ClassificationTestDataset(TEST_DIR, transforms = valid_transforms) #Why are we using val_transforms for Test Data?\n","# we are using val_transforms for Test Data to ensure that the test data is processed in a consistent manner, just like the validation data, and to avoid introducing any bias or artifacts during testing\n","# again, test data are unseen data, similar to validation data. So, we need to use vel_transform to have the same data format as validation data used to evaluate the model, we need to maintain the data consistency while evaluate model generalization \n","# in brief, same transfomation is needed for both test data and val data to ensure model evaluation is consistent, assess how well the model generalize to previously unseen data, and to ensure that the reported results are reliaibel and accurate to reflect model's performance on unseen data\n","test_loader = torch.utils.data.DataLoader(test_dataset, batch_size = config['batch_size'], shuffle = False,\n","                         drop_last = False, num_workers = 2)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:17:24.842638Z","iopub.status.busy":"2023-10-23T02:17:24.842333Z","iopub.status.idle":"2023-10-23T02:17:24.958530Z","shell.execute_reply":"2023-10-23T02:17:24.957592Z","shell.execute_reply.started":"2023-10-23T02:17:24.842613Z"},"id":"x4t8eU9gY0Jy","outputId":"062bbbcc-31fd-4974-ba39-f069a472cf3e","trusted":true},"outputs":[],"source":["print(\"Number of classes    : \", len(train_dataset.classes))\n","print(\"No. of train images  : \", train_dataset.__len__())\n","print(\"Shape of image       : \", train_dataset[0][0].shape)\n","print(\"Batch size           : \", config['batch_size'])\n","print(\"Train batches        : \", train_loader.__len__())\n","print(\"Val batches          : \", valid_loader.__len__())"]},{"cell_type":"markdown","metadata":{"id":"zs2Xw_tl0IQ8"},"source":["## Data visualization"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:17:24.960400Z","iopub.status.busy":"2023-10-23T02:17:24.959858Z","iopub.status.idle":"2023-10-23T02:17:32.149538Z","shell.execute_reply":"2023-10-23T02:17:32.148119Z","shell.execute_reply.started":"2023-10-23T02:17:24.960371Z"},"id":"xIoRUzCbz85y","outputId":"f84ce8fd-7f5a-49a8-8bb9-2187c861d62b","trusted":true},"outputs":[],"source":["# Visualize a few images in the dataset\n","# You can write your own code, and you don't need to understand the code\n","# It is highly recommended that you visualize your data augmentation as sanity check\n","\n","r, c    = [5, 5]\n","fig, ax = plt.subplots(r, c, figsize= (15, 15))\n","\n","k       = 0\n","dtl     = torch.utils.data.DataLoader(\n","    dataset     = torchvision.datasets.ImageFolder(TRAIN_DIR, transform= train_transforms), # dont wanna see the images with transforms\n","    batch_size  = config['batch_size'],\n","    shuffle     = True,\n",")\n","\n","for data in dtl:\n","    x, y = data\n","\n","    for i in range(r):\n","        for j in range(c):\n","            img = x[k].numpy().transpose(1, 2, 0)\n","            ax[i, j].imshow(img)\n","            ax[i, j].axis('off')\n","            k+=1\n","    break\n","\n","del dtl"]},{"cell_type":"markdown","metadata":{"id":"mIqmojPaWD0H"},"source":["# Very Simple Network (for Mandatory Early Submission)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class Squeeze_Excitation_Block(nn.Module):\n","    def __init__(self, in_channels, reduction_ratio=16):\n","        super(Squeeze_Excitation_Block, self).__init__()\n","        # Global average pooling layer\n","        self.glob_avg_pool_lay = nn.AdaptiveAvgPool2d(1)\n","        # First fully connected layer to reduce dimensionality\n","        self.fc_reduce = nn.Linear(in_channels, in_channels // reduction_ratio)\n","        # ReLU activation function\n","        self.relu = nn.ReLU(inplace=True)\n","        # Second fully connected layer to restore dimensionality\n","        self.fc_expand = nn.Linear(in_channels // reduction_ratio, in_channels)\n","        # Sigmoid activation function\n","        self.sigmoid = nn.Sigmoid()\n","\n","    def forward(self, x):\n","        # Global average pooling to obtain channel-wise statistics\n","        out_pooled = self.glob_avg_pool_lay(x).squeeze(-1).squeeze(-1)\n","        out_reduced = self.fc_reduce(out_pooled)\n","        out_activateed = self.relu(out_reduced)\n","        out_expanded= self.fc_expand(out_activateed)\n","        out_recal = self.sigmoid(out_expanded)\n","        out_recalibrated = out_recal.unsqueeze(-1).unsqueeze(-1)\n","        output = x * out_recalibrated \n","        return output\n","\n","class Residual_Block(nn.Module):\n","    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n","        super(Residual_Block, self).__init__()\n","        # First convolutional layer\n","        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n","        # Batch normalization after the first convolution\n","        self.bn1 = nn.BatchNorm2d(out_channels)\n","        # ReLU activation function\n","        self.relu = nn.ReLU(inplace=True)\n","        # Second convolutional layer\n","        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n","        # Batch normalization after the second convolution\n","        self.bn2 = nn.BatchNorm2d(out_channels)\n","        # Squeeze-Excitation block\n","        self.se_block = Squeeze_Excitation_Block(out_channels)\n","        # Downsample operation to match dimensions if needed\n","        self.downsample = downsample\n","\n","    def forward(self, x):\n","        residual = x\n","        out = self.conv1(x)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","        out = self.conv2(out)\n","        out = self.bn2(out)\n","        # Squeeze-Excitation block to recalibrate feature importance\n","        out = self.se_block(out)\n","        # When needed, adjust residual dimensions\n","        if self.downsample is not None:\n","            residual = self.downsample(x)\n","        # Add the residual and processed output\n","        out += residual\n","        out = self.relu(out)\n","\n","        return out\n","\n","class SEResNet(nn.Module):\n","    def __init__(self, block, layers, num_classes):\n","        super(SEResNet, self).__init__()\n","        self.in_channels = 64\n","        # Initial convolutional layer\n","        self.conv = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n","        # Batch normalization after the initial convolution\n","        self.bn = nn.BatchNorm2d(64)\n","        # ReLU activation function\n","        self.relu = nn.ReLU(inplace=True)\n","        # Max-pooling layer\n","        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n","\n","        # Residual blocks for each stage\n","        self.layer1 = self.createlayer(block, 64, layers[0])\n","        self.layer2 = self.createlayer(block, 128, layers[1], stride=2)\n","        self.layer3 = self.createlayer(block, 256, layers[2], stride=2)\n","        self.layer4 = self.createlayer(block, 512, layers[3], stride=2)\n","\n","        # Global average pooling layer\n","        self.glob_avg_pool_lay = nn.AdaptiveAvgPool2d(1)\n","        # Fully connected layer for classification\n","        self.fc_lay = nn.Linear(512, num_classes)\n","\n","    def createlayer(self, block, out_channels, blocks, stride=1):\n","        downsample = None\n","        if stride != 1 or self.in_channels != out_channels:\n","            # Downsample operation to match dimensions\n","            downsample = nn.Sequential(\n","                nn.Conv2d(self.in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n","                nn.BatchNorm2d(out_channels)\n","            )\n","\n","        layers = [block(self.in_channels, out_channels, stride, downsample)]\n","        self.in_channels = out_channels\n","\n","        for _ in range(1, blocks):\n","            layers.append(block(out_channels, out_channels))\n","\n","        return nn.Sequential(*layers)\n","\n","    def forward(self, x, return_feats=False):\n","        x = self.conv(x)\n","        x = self.bn(x)\n","        x = self.relu(x)\n","        x = self.maxpool(x)\n","\n","        x = self.layer1(x)\n","        x = self.layer2(x)\n","        x = self.layer3(x)\n","        x = self.layer4(x)\n","\n","        x = self.glob_avg_pool_lay(x)\n","        x = x.view(x.size(0), -1)\n","        output = self.fc_lay(x)\n","\n","        if return_feats:\n","            return output, x\n","        else:\n","            return output\n","\n","# Create an instance of the SEResNet model\n","model = SEResNet(Residual_Block, [4, 5, 6, 2], num_classes=7001).to(DEVICE)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# checking number of parameters used\n","number_parameters=[param.numel() for param in model.parameters() if param.requires_grad==True]\n","sum(number_parameters)"]},{"cell_type":"markdown","metadata":{"id":"KZCn0qHuZRKj"},"source":["# Setup everything for training"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:17:35.375606Z","iopub.status.busy":"2023-10-23T02:17:35.375186Z","iopub.status.idle":"2023-10-23T02:17:35.389176Z","shell.execute_reply":"2023-10-23T02:17:35.388415Z","shell.execute_reply.started":"2023-10-23T02:17:35.375573Z"},"id":"UowI9OcUYPjP","trusted":true},"outputs":[],"source":["criterion = torch.nn.CrossEntropyLoss(label_smoothing=0.2) # TODO: What loss do you need for a multi class classification problem?\n","optimizer = torch.optim.SGD(model.parameters(), lr=config['lr'], momentum=0.9, weight_decay=1e-4)\n","# TODO: Implement a scheduler (Optional but Highly Recommended)\n","# scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=3, factor=0.5, verbose=True)\n","scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=30, eta_min=0.00008)\n","# You can try ReduceLRonPlateau, StepLR, MultistepLR, CosineAnnealing, etc.\n","scaler = torch.cuda.amp.GradScaler() # Good news. We have FP16 (Mixed precision training) implemented for you\n","# It is useful only in the case of compatible GPUs such as T4/V100"]},{"cell_type":"markdown","metadata":{"id":"dzM11HtcboYv"},"source":["# Let's train!"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:17:35.390573Z","iopub.status.busy":"2023-10-23T02:17:35.390239Z","iopub.status.idle":"2023-10-23T02:17:35.405850Z","shell.execute_reply":"2023-10-23T02:17:35.405083Z","shell.execute_reply.started":"2023-10-23T02:17:35.390550Z"},"id":"bgSw6iJJavBZ","trusted":true},"outputs":[],"source":["def train(model, dataloader, optimizer, criterion):\n","\n","    model.train()\n","\n","    # Progress Bar\n","    batch_bar   = tqdm(total=len(dataloader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=5)\n","\n","    num_correct = 0\n","    total_loss  = 0\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        optimizer.zero_grad() # Zero gradients\n","\n","        images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","        with torch.cuda.amp.autocast(): # This implements mixed precision. Thats it!\n","            outputs = model(images)\n","            loss    = criterion(outputs, labels)\n","\n","        # Update no. of correct predictions & loss as we iterate\n","        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n","        total_loss      += float(loss.item())\n","\n","        # tqdm lets you add some details so you can monitor training as you train.\n","        batch_bar.set_postfix(\n","            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n","            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n","            num_correct = num_correct,\n","            lr          = \"{:.04f}\".format(float(optimizer.param_groups[0]['lr']))\n","        )\n","\n","        scaler.scale(loss).backward() # This is a replacement for loss.backward()\n","        scaler.step(optimizer) # This is a replacement for optimizer.step()\n","        scaler.update()\n","\n","        # TODO? Depending on your choice of scheduler,\n","\n","        # You may want to call some schdulers inside the train function. What are these?\n","\n","        batch_bar.update() # Update tqdm bar\n","\n","    batch_bar.close() # You need this to close the tqdm bar\n","\n","    acc         = 100 * num_correct / (config['batch_size']* len(dataloader))\n","    total_loss  = float(total_loss / len(dataloader))\n","\n","    return acc, total_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:17:35.407181Z","iopub.status.busy":"2023-10-23T02:17:35.406883Z","iopub.status.idle":"2023-10-23T02:17:35.420923Z","shell.execute_reply":"2023-10-23T02:17:35.420115Z","shell.execute_reply.started":"2023-10-23T02:17:35.407158Z"},"id":"m5V2UdnpdEoK","trusted":true},"outputs":[],"source":["def validate(model, dataloader, criterion):\n","\n","    model.eval()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=5)\n","\n","    num_correct = 0.0\n","    total_loss = 0.0\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        # Move images to device\n","        images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","        # Get model outputs\n","        with torch.inference_mode():\n","            outputs = model(images)\n","            loss = criterion(outputs, labels)\n","\n","        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n","        total_loss += float(loss.item())\n","\n","        batch_bar.set_postfix(\n","            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n","            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n","            num_correct=num_correct)\n","\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n","    total_loss = float(total_loss / len(dataloader))\n","    return acc, total_loss"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:17:35.422655Z","iopub.status.busy":"2023-10-23T02:17:35.422028Z","iopub.status.idle":"2023-10-23T02:17:35.595583Z","shell.execute_reply":"2023-10-23T02:17:35.594622Z","shell.execute_reply.started":"2023-10-23T02:17:35.422623Z"},"id":"cmotca6pcLLY","trusted":true},"outputs":[],"source":["gc.collect() # These commands help you when you face CUDA OOM error\n","torch.cuda.empty_cache()"]},{"cell_type":"markdown","metadata":{"id":"2mBgKGkXLrdJ"},"source":["# Wandb"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:17:35.597031Z","iopub.status.busy":"2023-10-23T02:17:35.596750Z","iopub.status.idle":"2023-10-23T02:17:38.310489Z","shell.execute_reply":"2023-10-23T02:17:38.309619Z","shell.execute_reply.started":"2023-10-23T02:17:35.597007Z"},"id":"Ix62_BkaLr_D","outputId":"ef167073-eb66-4056-f04d-c400b3c43770","trusted":true},"outputs":[],"source":["wandb.login(key=\"07b4b09ae74690496d4fe8aaf8e2230dc720df35\") #API Key is in your wandb account, under settings (wandb.ai/settings)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:17:38.312042Z","iopub.status.busy":"2023-10-23T02:17:38.311564Z","iopub.status.idle":"2023-10-23T02:17:41.216622Z","shell.execute_reply":"2023-10-23T02:17:41.215046Z","shell.execute_reply.started":"2023-10-23T02:17:38.312012Z"},"id":"VG0vmsmbRYEi","outputId":"c85d892e-b10a-4093-9a4a-0598ee731ef2","trusted":true},"outputs":[],"source":["# Create your wandb run\n","run = wandb.init(\n","    name = \"final_fin-submission\", ## Wandb creates random run names if you skip this field\n","    #     reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    id = \"bt064ps8\", ### Insert specific run id here if you want to resume a previous run\n","    resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"hw2p2-ablations\", ### Project should be created in your wandb account \n","    config = config ### Wandb Config for your run\n",")"]},{"cell_type":"markdown","metadata":{"id":"SQkRw1FvLqYe"},"source":["# Experiments"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:17:41.218023Z","iopub.status.busy":"2023-10-23T02:17:41.217715Z","iopub.status.idle":"2023-10-23T02:17:42.908883Z","shell.execute_reply":"2023-10-23T02:17:42.907426Z","shell.execute_reply.started":"2023-10-23T02:17:41.217997Z"},"trusted":true},"outputs":[],"source":["# path=\"/kaggle/input/cghnvjhb/checkpoint.pth\"\n","# checkpoint = torch.load(path)\n","# model.load_state_dict(checkpoint['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:17:42.910429Z","iopub.status.busy":"2023-10-23T02:17:42.910138Z","iopub.status.idle":"2023-10-23T02:18:17.969603Z","shell.execute_reply":"2023-10-23T02:18:17.966921Z","shell.execute_reply.started":"2023-10-23T02:17:42.910403Z"},"id":"EqWO8Edb0BK2","outputId":"db8278b1-a7f4-4685-f137-141697d46a1a","trusted":true},"outputs":[],"source":["best_valacc = 0.0\n","# root = '/kaggle/working/'\n","\n","# model_directory = os.path.join(root, \"models\")\n","\n","for epoch in range(config['epochs']):\n","\n","    curr_lr = float(optimizer.param_groups[0]['lr'])\n","\n","    train_acc, train_loss = train(model, train_loader, optimizer, criterion)\n","\n","    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t Learning Rate {:.04f}\".format(\n","        epoch + 1,\n","        config['epochs'],\n","        train_acc,\n","        train_loss,\n","        curr_lr))\n","\n","    val_acc, val_loss = validate(model, valid_loader, criterion)\n","\n","    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\".format(val_acc, val_loss))\n","\n","    wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc,\n","               'validation_loss': val_loss, \"learning_Rate\": curr_lr})\n","\n","    # If you are using a scheduler in your train function within your iteration loop, you may want to log\n","    # your learning rate differently\n","    scheduler.step()\n","\n","    # #Save model in drive location if val_acc is better than best recorded val_acc\n","    if val_acc >= best_valacc:\n","        #path = os.path.join(root, model_directory, 'checkpoint' + '.pth')\n","        print(\"Saving model\")\n","        torch.save({'model_state_dict':model.state_dict(),\n","                  'optimizer_state_dict':optimizer.state_dict(),\n","                  'scheduler_state_dict':scheduler.state_dict(),\n","                  'val_acc': val_acc,\n","                  'epoch': epoch}, './checkpoint.pth')\n","        best_valacc = val_acc\n","        wandb.save(path)\n","      # You may find it interesting to exlplore Wandb Artifcats to version your models\n","run.finish()"]},{"cell_type":"markdown","metadata":{"id":"UpgCHImRkYQW"},"source":["# Classification Task: Testing"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:18:27.346670Z","iopub.status.busy":"2023-10-23T02:18:27.346292Z","iopub.status.idle":"2023-10-23T02:18:27.357679Z","shell.execute_reply":"2023-10-23T02:18:27.356424Z","shell.execute_reply.started":"2023-10-23T02:18:27.346642Z"},"id":"U2WQEUjXkWvo","trusted":true},"outputs":[],"source":["def test(model, dataloader):\n","    model.eval()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Test')\n","    test_results = []\n","\n","    for i, (images) in enumerate(dataloader):\n","        # TODO: Finish predicting on the test set.\n","        images = images.to(DEVICE)\n","\n","        # Set the model to evaluation mode and use torch.no_grad()\n","        with torch.no_grad():\n","            outputs = model(images)\n","\n","        outputs = torch.argmax(outputs, axis=1).detach().cpu().numpy().tolist()\n","        test_results.extend(outputs)\n","\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","    return test_results\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-23T02:18:17.972987Z","iopub.status.idle":"2023-10-23T02:18:17.973661Z","shell.execute_reply":"2023-10-23T02:18:17.973478Z","shell.execute_reply.started":"2023-10-23T02:18:17.973450Z"},"id":"K7R1lcCAzULc","outputId":"603e5e43-8a6c-4ae9-b9b5-0349a1b44e18","trusted":true},"outputs":[],"source":["test_results = test(model, test_loader)"]},{"cell_type":"markdown","metadata":{"id":"zqfUzwS2L1gx"},"source":["## Generate csv to submit to Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-23T02:18:17.974929Z","iopub.status.idle":"2023-10-23T02:18:17.975637Z","shell.execute_reply":"2023-10-23T02:18:17.975388Z","shell.execute_reply.started":"2023-10-23T02:18:17.975350Z"},"id":"Vob9a2-HkW_V","trusted":true},"outputs":[],"source":["with open(\"classification_early_submission.csv\", \"w+\") as f:\n","    f.write(\"id,label\\n\")\n","    for i in range(len(test_dataset)):\n","        f.write(\"{},{}\\n\".format(str(i).zfill(6) + \".jpg\", test_results[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-23T02:18:17.977157Z","iopub.status.idle":"2023-10-23T02:18:17.977549Z","shell.execute_reply":"2023-10-23T02:18:17.977387Z","shell.execute_reply.started":"2023-10-23T02:18:17.977349Z"},"id":"GnRUN53CZMTf","outputId":"93e9c55e-7507-495d-b34c-8cf1f69c9550","trusted":true},"outputs":[],"source":["# !kaggle competitions submit -c 11-785-f23-hw2p2-classification -f classification_early_submission.csv -m \"early submission\""]},{"cell_type":"markdown","metadata":{"id":"7WYgUjJzUiGU"},"source":["# Verification Task: Validation"]},{"cell_type":"markdown","metadata":{"id":"FoBFFF8-Lpvj"},"source":["The verification task consists of the following generalized scenario:\n","- You are given X unknown identitites\n","- You are given Y known identitites\n","- Your goal is to match X unknown identities to Y known identities.\n","\n","We have given you a verification dataset, that consists of 960 known identities, and 1080 unknown identities. The 1080 unknown identities are split into dev (360) and test (720). Your goal is to compare the unknown identities to the 1080 known identities and assign an identity to each image from the set of unknown identities. Some unknown identities do not have correspondence in known identities, you also need to identify these and label them with a special label n000000.\n","\n","Your will use/finetune your model trained for classification to compare images between known and unknown identities using a similarity metric and assign labels to the unknown identities.\n","\n","This will judge your model's performance in terms of the quality of embeddings/features it generates on images/faces it has never seen during training for classification."]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:18:37.690557Z","iopub.status.busy":"2023-10-23T02:18:37.689933Z","iopub.status.idle":"2023-10-23T02:18:52.843851Z","shell.execute_reply":"2023-10-23T02:18:52.842425Z","shell.execute_reply.started":"2023-10-23T02:18:37.690521Z"},"id":"f9aY5o-suWdn","outputId":"9e8906e8-9ddc-4cae-f1a6-e54a17b4f371","trusted":true},"outputs":[],"source":["# This obtains the list of known identities from the known folder\n","known_regex = \"/kaggle/working/11-785-f23-hw2p2-verification/known/*/*\"\n","known_paths = [i.split('/')[-2] for i in sorted(glob.glob(known_regex))]\n","\n","# Obtain a list of images from unknown folders\n","unknown_dev_regex = \"/kaggle/working/11-785-f23-hw2p2-verification/unknown_dev/*\"\n","unknown_test_regex = \"/kaggle/working/11-785-f23-hw2p2-verification/unknown_test/*\"\n","\n","# We load the images from known and unknown folders\n","unknown_dev_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_dev_regex)))]\n","unknown_test_images = [Image.open(p) for p in tqdm(sorted(glob.glob(unknown_test_regex)))]\n","known_images = [Image.open(p) for p in tqdm(sorted(glob.glob(known_regex)))]\n","\n","# Why do you need only ToTensor() here?\n","# we need to change the images in original format to PyTorch's tensor format \n","# to ensure that all the image data is in a consistent data type, which is necessary for further processing\n","# during cosine similarity computations between images as we are computing similarity (distance) between images and \n","# ToTensor() is a prerequisite for using Cosine Similarity\n","transforms = torchvision.transforms.Compose([\n","    torchvision.transforms.ToTensor()])\n","\n","unknown_dev_images = torch.stack([transforms(x) for x in unknown_dev_images])\n","unknown_test_images = torch.stack([transforms(x) for x in unknown_test_images])\n","known_images  = torch.stack([transforms(y) for y in known_images ])\n","#Print your shapes here to understand what we have done\n","print('unknown_dev_images\\t', unknown_dev_images.shape)\n","print('unknown_test_images\\t', unknown_test_images.shape)\n","print('known_images\\t', known_images.shape)\n","# You can use other similarity metrics like Euclidean Distance if you wish\n","similarity_metric = torch.nn.CosineSimilarity(dim= 1, eps= 1e-6)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:18:53.956764Z","iopub.status.busy":"2023-10-23T02:18:53.956403Z","iopub.status.idle":"2023-10-23T02:18:53.967294Z","shell.execute_reply":"2023-10-23T02:18:53.966176Z","shell.execute_reply.started":"2023-10-23T02:18:53.956734Z"},"trusted":true},"outputs":[],"source":["class CenterLoss(nn.Module):\n","    \"\"\"Center Loss\n","    Center Loss Paper:\n","    https://ydwen.github.io/papers/WenECCV16.pdf\n","    Args:\n","        num_classes (int): The number of classes for your model.\n","        feat_dim (int): The dimension of your output feature.\n","    \"\"\"\n","    def __init__(self, num_classes=7001, feat_dim=512):\n","        super(CenterLoss, self).__init__()\n","        self.num_classes = num_classes\n","        self.feat_dim = feat_dim\n","\n","        # Initialize centers for each class.\n","        # The centers are learnable parameters, and you need to use the nn.Parameter\n","        # so that they are registered as model parameters.\n","        # We initialize them using random values, and they are moved to GPU.\n","        self.centers = nn.Parameter(torch.randn(self.num_classes, self.feat_dim).cuda())\n","\n","    def forward(self, x, labels):\n","        \"\"\"\n","        Args:\n","            x: Feature matrix with shape (batch_size, feat_dim).\n","            labels: Ground truth labels with shape (batch_size).\n","        \"\"\"\n","        # Broadcast the centers for each input based on the labels.\n","        # This will create a tensor where centers[i] will contain the center of the true label of x[i].\n","        centers_batch = self.centers[labels]\n","\n","        # Calculate the squared Euclidean distances between inputs and current centers.\n","        dist = torch.sum((x - centers_batch) ** 2, dim=1)\n","\n","        # Clamp the distances to avoid NaN in log and to provide numerical stability.\n","        dist = torch.clamp(dist, min=1e-12, max=1e+12)\n","\n","        # Calculate the mean loss across the batch.\n","        loss = torch.mean(dist)\n","\n","        return loss\n"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:18:58.857135Z","iopub.status.busy":"2023-10-23T02:18:58.856775Z","iopub.status.idle":"2023-10-23T02:18:58.895294Z","shell.execute_reply":"2023-10-23T02:18:58.894418Z","shell.execute_reply.started":"2023-10-23T02:18:58.857105Z"},"trusted":true},"outputs":[],"source":["# Initialize the CenterLoss and its optimizer\n","center_loss = CenterLoss(num_classes=7001, feat_dim=512)\n","optimizer_center_loss = torch.optim.SGD(center_loss.parameters(), lr=0.1)\n","scheduler_center_loss = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_center_loss, T_max=30, eta_min=0.001)"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:19:04.191170Z","iopub.status.busy":"2023-10-23T02:19:04.190821Z","iopub.status.idle":"2023-10-23T02:19:04.209941Z","shell.execute_reply":"2023-10-23T02:19:04.208622Z","shell.execute_reply.started":"2023-10-23T02:19:04.191139Z"},"trusted":true},"outputs":[],"source":["def train(model: nn.Module, \n","          train_loader: torch.utils.data.DataLoader, \n","          optimizer: torch.optim.Optimizer, \n","          optimizer_center_loss: torch.optim.Optimizer, \n","          criterion: nn.Module, \n","          fine_tuning_loss: nn.Module,  # Center Loss as fine_tuning_loss\n","          loss_weight, \n","          \n","          scaler: torch.cuda.amp.GradScaler, \n","          device):\n","    \n","    model.train()\n","    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train', ncols=6)\n","\n","    num_correct = 0\n","    total_loss_ft  = 0\n","    total_loss  = 0\n","    \n","    for i, (images, labels) in enumerate(train_loader):\n","        \n","        optimizer.zero_grad()\n","        optimizer_center_loss.zero_grad()\n","        \n","        images, labels = images.to(device), labels.to(device)\n","\n","        with torch.cuda.amp.autocast():\n","            outputs, feats = model(images, return_feats=True)\n","            loss0 = criterion(outputs, labels)  # Calculate cross-entropy loss\n","            loss1 = fine_tuning_loss(feats, labels) * loss_weight  # Calculate weighted fine-tuning loss (Center Loss)\n","            \n","            \n","        # Update no. of correct predictions & loss as we iterate\n","        num_correct     += int((torch.argmax(outputs, axis=1) == labels).sum())\n","        total_loss      += float(loss0.item())\n","        total_loss_ft      += float(loss1.item())\n","        \n","        batch_bar.set_postfix(\n","            acc         = \"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n","            loss        = \"{:.04f}\".format(float(total_loss / (i + 1))),\n","            loss_ft        = \"{:.04f}\".format(float(total_loss_ft / (i + 1))),\n","            num_correct = num_correct,\n","            lr          = \"{:.04f}\".format(float(optimizer_center_loss.param_groups[0]['lr']))\n","        )\n","        \n","        \n","\n","        scaler.scale(loss0).backward(retain_graph=True)  # Backward pass for the classification loss\n","        scaler.scale(loss1).backward()  # Backward pass for the fine-tuning loss\n","        \n","        # update fine tuning loss' parameters\n","        # the paramerters should be adjusted according to the loss_weight you choose\n","        for parameter in fine_tuning_loss.parameters():\n","            parameter.grad.data *= (1.0 / loss_weight)\n","\n","        scaler.step(optimizer_center_loss)  # Step optimizer for fine-tuning loss\n","        scaler.step(optimizer)  # Step optimizer for classification loss\n","        scaler.update()\n","    \n","        \n","        batch_bar.update() # Update tqdm bar\n","        \n","        del images, labels, outputs, loss0, loss1\n","        torch.cuda.empty_cache()\n","\n","    batch_bar.close() # You need this to close the tqdm bar\n","\n","    acc         = 100 * num_correct / (config['batch_size']* len(train_loader))\n","    total_loss  = float(total_loss / len(train_loader))\n","    total_loss_ft  = float(total_loss_ft / len(train_loader))\n","    \n","    return acc, total_loss, total_loss_ft"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:19:09.879793Z","iopub.status.busy":"2023-10-23T02:19:09.879148Z","iopub.status.idle":"2023-10-23T02:19:09.892159Z","shell.execute_reply":"2023-10-23T02:19:09.891080Z","shell.execute_reply.started":"2023-10-23T02:19:09.879760Z"},"trusted":true},"outputs":[],"source":["def validate_l(model, dataloader, criterion, fine_tuning_loss, loss_weight):\n","\n","    model.eval()\n","    batch_bar = tqdm(total=len(dataloader), dynamic_ncols=True, position=0, leave=False, desc='Val', ncols=6)\n","\n","    num_correct = 0.0\n","    total_loss = 0.0\n","    total_loss_ft = 0.0\n","\n","    for i, (images, labels) in enumerate(dataloader):\n","\n","        # Move images to device\n","        images, labels = images.to(DEVICE), labels.to(DEVICE)\n","\n","        # Get model outputs\n","        with torch.inference_mode():\n","            outputs, feats = model(images, return_feats=True)\n","            loss = criterion(outputs, labels)\n","            loss_ft = fine_tuning_loss(feats, labels)* loss_weight\n","\n","        num_correct += int((torch.argmax(outputs, axis=1) == labels).sum())\n","        total_loss += float(loss.item())\n","        total_loss_ft      += float(loss_ft.item())\n","\n","        batch_bar.set_postfix(\n","            acc=\"{:.04f}%\".format(100 * num_correct / (config['batch_size']*(i + 1))),\n","            loss=\"{:.04f}\".format(float(total_loss / (i + 1))),\n","            loss_ft=\"{:.04f}\".format(float(total_loss_ft / (i + 1))),\n","            num_correct=num_correct)\n","\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","    acc = 100 * num_correct / (config['batch_size']* len(dataloader))\n","    total_loss = float(total_loss / len(dataloader))\n","    total_loss_ft  = float(total_loss_ft / len(dataloader))\n","    return acc, total_loss, total_loss_ft"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-23T02:18:17.989425Z","iopub.status.idle":"2023-10-23T02:18:17.989863Z","shell.execute_reply":"2023-10-23T02:18:17.989653Z","shell.execute_reply.started":"2023-10-23T02:18:17.989633Z"},"trusted":true},"outputs":[],"source":["wandb.login(key=\"07b4b09ae74690496d4fe8aaf8e2230dc720df35\")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-23T02:18:17.991396Z","iopub.status.idle":"2023-10-23T02:18:17.991864Z","shell.execute_reply":"2023-10-23T02:18:17.991647Z","shell.execute_reply.started":"2023-10-23T02:18:17.991625Z"},"trusted":true},"outputs":[],"source":["# Create your wandb run\n","run = wandb.init(\n","    name = \"final_fin-submission\", ## Wandb creates random run names if you skip this field\n","    #     reinit = True, ### Allows reinitalizing runs when you re-run this cell\n","    id = \"bt064ps8\", ### Insert specific run id here if you want to resume a previous run\n","    resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n","    project = \"hw2p2-ablations\", ### Project should be created in your wandb account \n","    config = config ### Wandb Config for your run\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:23:43.981614Z","iopub.status.busy":"2023-10-23T02:23:43.981211Z","iopub.status.idle":"2023-10-23T02:23:44.171412Z","shell.execute_reply":"2023-10-23T02:23:44.170206Z","shell.execute_reply.started":"2023-10-23T02:23:43.981577Z"},"trusted":true},"outputs":[],"source":["# path =\"/kaggle/input/cghnvjhb/checkpoint.pth\"\n","# model.load_state_dict(torch.load(path)['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T02:23:47.668435Z","iopub.status.busy":"2023-10-23T02:23:47.668077Z","iopub.status.idle":"2023-10-23T03:02:20.157221Z","shell.execute_reply":"2023-10-23T03:02:20.156401Z","shell.execute_reply.started":"2023-10-23T02:23:47.668407Z"},"trusted":true},"outputs":[],"source":["loss_weight = 0.002\n","\n","best_loss = 2.5\n","\n","for epoch in range(config['epochs']):\n","\n","    curr_lr = float(optimizer_center_loss.param_groups[0]['lr'])\n","\n","    train_acc, train_loss, ft_loss = train(model, train_loader, optimizer, optimizer_center_loss, criterion, center_loss, loss_weight, scaler, DEVICE)\n","\n","\n","    print(\"\\nEpoch {}/{}: \\nTrain Acc {:.04f}%\\t Train Loss {:.04f}\\t FT train loss {:.04f}\\t Learning Rate ft {:.04f}\".format(\n","        epoch + 1,\n","        config['epochs'],\n","        train_acc,\n","        train_loss,\n","        ft_loss,\n","        curr_lr))\n","\n","    val_acc, val_loss, val_loss_ft = validate_l(model, valid_loader, criterion, center_loss, loss_weight)\n","\n","    print(\"Val Acc {:.04f}%\\t Val Loss {:.04f}\\t FT Val Loss {:.04f}\".format(val_acc, val_loss, val_loss_ft))\n","\n","    wandb.log({\"train_loss\":train_loss, 'train_Acc': train_acc, 'validation_Acc':val_acc, 'ft_loss' :ft_loss,\n","               'validation_loss': val_loss, \"learning_Rate_ft\": curr_lr})\n","\n","    # If you are using a scheduler in your train function within your iteration loop, you may want to log\n","    # your learning rate differently\n","    \n","    scheduler_center_loss.step()\n","\n","    # #Save model in drive location if val_acc is better than best recorded val_acc\n","    if val_loss_ft <= best_loss:\n","#         path = os.path.join(root, model_directory, 'checkpoint.pth')\n","        \n","        print(\"Saving model\")\n","        torch.save({'model_state_dict':model.state_dict(),\n","                  'optimizer_state_dict':optimizer.state_dict(),\n","                  'optimizer_center_loss_state_dict':optimizer_center_loss.state_dict(),\n","                  'scheduler_center_loss_state_dict':scheduler_center_loss.state_dict(),\n","                  'ft_loss': ft_loss,\n","                  'epoch': epoch}, './checkpoint.pth')\n","        best_loss = val_loss_ft\n","        wandb.save(path)\n","      # You may find it interesting to exlplore Wandb Artifcats to version your models\n","run.finish()"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T03:31:58.097750Z","iopub.status.busy":"2023-10-23T03:31:58.097279Z","iopub.status.idle":"2023-10-23T03:31:58.273637Z","shell.execute_reply":"2023-10-23T03:31:58.272626Z","shell.execute_reply.started":"2023-10-23T03:31:58.097703Z"},"trusted":true},"outputs":[],"source":["# path =\"/kaggle/input/ghgghbjhb/checkpoint.pth\"\n","# model.load_state_dict(torch.load(path)['model_state_dict'])"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T03:32:01.688880Z","iopub.status.busy":"2023-10-23T03:32:01.688541Z","iopub.status.idle":"2023-10-23T03:32:01.703384Z","shell.execute_reply":"2023-10-23T03:32:01.702409Z","shell.execute_reply.started":"2023-10-23T03:32:01.688855Z"},"trusted":true},"outputs":[],"source":["def eval_verification(unknown_images, known_images, model, similarity, batch_size= config['batch_size'], mode='val'):\n","\n","    unknown_feats, known_feats = [], []\n","\n","    batch_bar = tqdm(total=len(unknown_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n","    model.eval()\n","\n","    # We load the images as batches for memory optimization and avoiding CUDA OOM errors\n","    for i in range(0, unknown_images.shape[0], batch_size):\n","        unknown_batch = unknown_images[i:i+batch_size] # Slice a given portion upto batch_size\n","\n","        with torch.no_grad():\n","            _, unknown_feat = model(unknown_batch.float().to(DEVICE), return_feats=True) #Get features from model\n","        unknown_feats.append(unknown_feat)\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","\n","    batch_bar = tqdm(total=len(known_images)//batch_size, dynamic_ncols=True, position=0, leave=False, desc=mode)\n","\n","    for i in range(0, known_images.shape[0], batch_size):\n","        known_batch = known_images[i:i+batch_size]\n","        with torch.no_grad():\n","              _, known_feat = model(known_batch.float().to(DEVICE), return_feats=True)\n","\n","        known_feats.append(known_feat)\n","        batch_bar.update()\n","\n","    batch_bar.close()\n","\n","    # Concatenate all the batches\n","    unknown_feats = torch.cat(unknown_feats, dim=0)\n","    known_feats = torch.cat(known_feats, dim=0)\n","\n","    similarity_values = torch.stack([similarity(unknown_feats, known_feature) for known_feature in known_feats])\n","    # Print the inner list comprehension in a separate cell - what is really happening?\n","\n","    max_similarity_values, predictions = similarity_values.max(0) #Why are we doing an max here, where are the return values?\n","    # we want to identify the target image that is most similar to the reference image\n","    # as we are doing image verification task, by finding the maximum similarity value and its corresponding index (prediction), \n","    # you can determine which target image is the closest match to the reference image\n","    # therefore, the return values, max_similarity_values and predictions, will be used for subsequent processing or decision-making \n","    # based on the identified most similar target image.\n","    max_similarity_values, predictions = max_similarity_values.cpu().numpy(), predictions.cpu().numpy()\n","\n","    # Note that in unknown identities, there are identities without correspondence in known identities.\n","    # Therefore, these identities should be not similar to all the known identities, i.e. max similarity will be below a certain\n","    # threshold compared with those identities with correspondence.\n","\n","    # In early submission, you can ignore identities without correspondence, simply taking identity with max similarity value\n","    # pred_id_strings = [known_paths[i] for i in predictions] # Map argmax indices to identity strings\n","\n","    # After early submission, remove the previous line and uncomment the following code\n","\n","    threshold = 0.5 # Choose a proper threshold\n","    NO_CORRESPONDENCE_LABEL = 'n000000'\n","    pred_id_strings = []\n","    for idx, prediction in enumerate(predictions):\n","        if max_similarity_values[idx] < threshold: # why < ? Think about what is your similarity metric\n","            pred_id_strings.append(NO_CORRESPONDENCE_LABEL)\n","        else:\n","            pred_id_strings.append(known_paths[prediction])\n","\n","    if mode == 'val':\n","      true_ids = pd.read_csv('/kaggle/input/11-785-f23-hw2p2-verification/11-785-f23-hw2p2-verification/verification_dev.csv')['label'].tolist()\n","      accuracy = accuracy_score(pred_id_strings, true_ids)\n","      print(\"Verification Accuracy = {}\".format(accuracy))\n","\n","    return pred_id_strings"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T03:32:02.223883Z","iopub.status.busy":"2023-10-23T03:32:02.223048Z","iopub.status.idle":"2023-10-23T03:32:05.192137Z","shell.execute_reply":"2023-10-23T03:32:05.191404Z","shell.execute_reply.started":"2023-10-23T03:32:02.223848Z"},"id":"zMC7FacaUnJ7","outputId":"a876886a-b94e-4ae5-94db-d981129680e7","trusted":true},"outputs":[],"source":["# verification eval\n","pred_id_strings = eval_verification(unknown_dev_images, known_images, model, similarity_metric, config['batch_size'], mode='val')\n","# verification test\n","pred_id_strings = eval_verification(unknown_test_images, known_images, model, similarity_metric, config['batch_size'], mode='test')"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.status.busy":"2023-10-23T02:18:18.003837Z","iopub.status.idle":"2023-10-23T02:18:18.004189Z","shell.execute_reply":"2023-10-23T02:18:18.004033Z","shell.execute_reply.started":"2023-10-23T02:18:18.004016Z"},"id":"1bCmzKfH7XgG","trusted":true},"outputs":[],"source":["# add your finetune/retrain code here"]},{"cell_type":"markdown","metadata":{"id":"iTLW0RPD7XGC"},"source":["## Generate csv to submit to Kaggle"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T03:32:09.019312Z","iopub.status.busy":"2023-10-23T03:32:09.018467Z","iopub.status.idle":"2023-10-23T03:32:09.025668Z","shell.execute_reply":"2023-10-23T03:32:09.024752Z","shell.execute_reply.started":"2023-10-23T03:32:09.019279Z"},"id":"fD-r-HmsAeWV","trusted":true},"outputs":[],"source":["with open(\"verification_early_submission.csv\", \"w+\") as f:\n","    f.write(\"id,label\\n\")\n","    for i in range(len(pred_id_strings)):\n","        f.write(\"{},{}\\n\".format(i, pred_id_strings[i]))"]},{"cell_type":"code","execution_count":null,"metadata":{"execution":{"iopub.execute_input":"2023-10-23T03:32:13.205559Z","iopub.status.busy":"2023-10-23T03:32:13.205192Z","iopub.status.idle":"2023-10-23T03:32:18.111414Z","shell.execute_reply":"2023-10-23T03:32:18.110204Z","shell.execute_reply.started":"2023-10-23T03:32:13.205529Z"},"id":"jPIgq0tMZ8qk","outputId":"085870d2-874a-4ac5-de58-3a2d485255b4","trusted":true},"outputs":[],"source":["# !kaggle competitions submit -c 11-785-f23-hw2p2-verification -f verification_early_submission.csv -m \"early submission\""]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":4}
